#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Calculate empirical covariance from data and fit covariance model.

Example:
    python covxy.py ~/data/ers1/floating/filt_scat_det/joined_pts_a.h5_ross -o junk.h5 -v lon lat h_res

"""

import os
import sys
import h5py
import pyproj
import argparse
import numpy as np
from time import time as tim
import matplotlib.pyplot as plt
from scipy.optimize import curve_fit
from numba import jit, int32, float64
from scipy.spatial.distance import cdist, pdist, squareform


def get_args():
    """ Get command-line arguments. """

    des = 'Optimal Interpolation of spatial data'
    parser = argparse.ArgumentParser(description=des)

    parser.add_argument(
            'ifile', metavar='ifile', type=str, nargs='+',
            help='name of i-file, numpy binary or ascii (for binary ".npy")')

    parser.add_argument(
            '-o', metavar='ofile', dest='ofile', type=str, nargs=1,
            help='name of o-file, numpy binary or ascii (for binary ".npy")',
            default=[None])

    parser.add_argument(
            '-v', metavar=('time', 'lon','lat', 'obs'),
            dest='vnames', type=str, nargs=4,
            help=('name of t/x/y/z variables in the HDF5'),
            default=[None], required=True)

    parser.add_argument(
            '-r', metavar=('radius'), dest='radius', type=float, nargs=1,
            help=('search radius in km'),
            default=[10],)

    parser.add_argument(
            '-t', metavar=('t1', 't2'), dest='tspan', type=float, nargs=2,
            help=('time span to subest for covariance calc'),
            default=[None],)

    parser.add_argument(
            '-d', metavar=('dt'), dest='dt', type=float, nargs=1,
            help=('max time separation between pts'),
            default=[None],)

    parser.add_argument(
            '-m', metavar=('samples'), dest='samples', type=int, nargs=1,
            help=('number of time sub-samples (of length dt)'),
            default=[None],)

    parser.add_argument(
            '-x', metavar=('dmin', 'dmax'), dest='dspan', type=float, nargs=2,
            help=('distance span to calculate sample covariances'),
            default=[0,10],)

    parser.add_argument(
            '-l', metavar=('dx'), dest='dx', type=float, nargs=1,
            help=('interval to defina a distance class'),
            default=[1],)

    parser.add_argument(
            '-p', metavar=('epsg_num'), dest='proj', type=str, nargs=1,
            help=('EPSG proj number (AnIS=3031, GrIS=3413)'),
            default=['3031'],)

    return parser.parse_args()


def print_args(args):
    print 'Input arguments:'
    for arg in vars(args).iteritems():
        print arg


""" Generic functions. """

def transform_coord(proj1, proj2, x, y):
    """
    Transform coordinates from proj1 to proj2 (EPSG num).

    Examples EPSG proj:
        Geodetic (lon/lat): 4326
        Stereo AnIS (x/y):  3031
        Stereo GrIS (x/y):  3413
    """
    # Set full EPSG projection strings
    proj1 = pyproj.Proj("+init=EPSG:"+str(proj1))
    proj2 = pyproj.Proj("+init=EPSG:"+str(proj2))
    # Convert coordinates
    return pyproj.transform(proj1, proj2, x, y)


def get_grid(xmin, xmax, ymin, ymax, dx, dy):
    """ Generate a regular grid. """

    # Setup grid dimensions
    Nx = int((np.abs(xmax - xmin)) / dx) + 1
    Ny = int((np.abs(ymax - ymin)) / dy) + 1

    # Initiate lat/lon vectors for grid
    x = np.linspace(xmin, xmax, Nx)
    y = np.linspace(ymin, ymax, Ny)

    # Construct output grid-coordinates
    xx, yy = np.meshgrid(x, y)

    # Flatten prediction grid
    return xx.ravel(), yy.ravel()

#-----------------------------------------------------------

""" Compiled functions. """

@jit(nopython=True)
def i_index(n, k):
    return int(n - (4.*n**2 - 4*n - 8*k + 1)**.5/2 - .5)


@jit(nopython=True)
def j_index(n, k, i):
    return int(k + i * (i + 3 - 2*n)/2 + 1)


@jit(nopython=True)
def get_pair_indices(n, kk):
    """
    Map k-index of condensed mat -> i,j-indices of distance mat.

    n : number of points (dist_mat.shape[0]).
    kk : indices of elements in condensed matrix. 
    """
    N = kk.shape[0]
    ii = np.empty(N, int32)
    jj = np.empty(N, int32)
    for m in range(N):
        k = kk[m]
        i = i_index(n, k)
        j = j_index(n, k, i)
        ii[m] = i
        jj[m] = j
    return ii, jj


@jit(nopython=True)
def get_dist_indices(dist, lag, tol):
    """ Find indices of distances within lag +/- tol. """
    l1 = lag-tol
    l2 = lag+tol
    N = dist.shape[0]
    idx = np.empty(N, int32)
    k = 0
    for i in range(N):
        d = dist[i]
        if d >= l1 and d < l2:
            idx[k] = i
            k = k + 1
    return idx[:k]


@jit(nopython=True)
def covxy(x, y):
    """ Covariance: average of pair products. """
    N = x.shape[0]
    psum = 0
    for i in range(N):
        psum = psum + x[i] * y[i]
    return psum / N


@jit(nopython=True)
def distcov(x, y, dists, lags, tol):
    """
    Calculate sample distance convariance.

    x, y: pair of values to compute covariance.
    dists: distances between x and y (condensed mat).
    lags: discrete lag values to estimate sample cov.
    tol: half-width of the lag interval (lag +/- tol).
    """
    n_pairs = x.shape[0]
    n_lags = lags.shape[0]
    cov = np.empty(n_lags, float64)

    for i_lag in range(n_lags):
        
        lag = lags[i_lag]

        kk = get_dist_indices(dists, lag, tol)

        if len(kk) == 0:
            cov[i_lag] = np.nan
            continue

        ii, jj = get_pair_indices(n_pairs, kk)

        cov[i_lag] = covxy(x[ii], y[jj])

    return cov

#-----------------------------------------------------------

""" Covariance models. """

def gauss(r, s, R):
    return s**2 * np.exp(-r**2/R**2)

def markov(r, s, R):
    return s**2 * (1 + r/R) * np.exp(-r/R)

def generic(r, s, R):
    return s * (1 + (r/R) - 0.5 * (r/R)**2) * np.exp(-r/R)

#----------------------------------------------------------

""" Helper functions. """

from scipy.optimize import least_squares

def model_fit(model, x, y, p0=[1.,1.]):

    # Function computing the residulas
    fun = lambda p, x, y: model(x, *p) - y

    # Iterative robust minimization of residuals
    fit = least_squares(fun, p0, args=(x,y), loss='soft_l1', f_scale=0.1)

    # Return fitted params
    return fit.x, fit.fun


def filter_time(t_, x_, y_, z_, tspan=(1995.25, 1995.5)):
    """ Reduce domain size for testing purposes. """
    t, x, y, z = t_.copy(), x_.copy(), y_.copy(), z_.copy()
    ii, = np.where((t < tspan[0]) | (t > tspan[1]))
    t[ii] = np.nan
    x[ii] = np.nan
    y[ii] = np.nan
    z[ii] = np.nan
    return t, x, y, z


def filter_domain(t_, x_, y_, z_, radius=1000):
    """ Reduce domain size for testing purposes. """
    t, x, y, z = t_.copy(), x_.copy(), y_.copy(), z_.copy()
    x0 = np.nanmean(x)
    y0 = np.nanmean(y)
    xx = (x < x0-radius) | (x > x0+radius)
    yy = (y < y0-radius) | (y > y0+radius)
    ii, = np.where(xx | yy)
    t[ii] = np.nan
    x[ii] = np.nan
    y[ii] = np.nan
    z[ii] = np.nan
    return t, x, y, z


def filter_invalid(t_, x_, y_, z_):
    """ Mask NaNs and Zeros. """
    t, x, y, z = t_.copy(), x_.copy(), y_.copy(), z_.copy()
    ii, = np.where((z == 0) | np.isnan(z))
    t[ii] = np.nan
    x[ii] = np.nan
    y[ii] = np.nan
    z[ii] = np.nan
    return t, x, y, z


def remove_nan(t, x, y, z):
    """ Remove NaNs. """
    ii, = np.where(~np.isnan(z))
    return t[ii], x[ii], y[ii], z[ii]


# Parser argument to variable
args = get_args() 

# Read input from terminal
ifile = args.ifile[0]
ofile = args.ofile[0]
tvar = args.vnames[0]
xvar = args.vnames[1]
yvar = args.vnames[2]
zvar = args.vnames[3]
radius = args.radius[0] * 1e3  # km -> m
tspan = args.tspan[:]
dt = args.dt[0]
samples = args.samples[0]
proj = args.proj[0]
dmin = args.dspan[0] * 1e3  # km -> m
dmax = args.dspan[1] * 1e3  # km -> m
dx = args.dx[0] * 1e3  # km -> m

# Print parameters to screen
print_args(args)


print "reading input file ..."

with h5py.File(ifile, 'r') as f:

    step = 2
    time = f[tvar][::step]
    lon = f[xvar][::step]
    lat = f[yvar][::step]
    obs = f[zvar][::step]

    if 1:
        # Remove uncorrected data (this should be done before applying this code?)
        b = f['h_bs'][::step] 
        obs[b==0] = np.nan
        obs[np.isnan(b)] = np.nan

if None in tspan:
    tspan = [np.nanmin(time), np.nanmax(time)]

if dt is not None:
    tspans = [(t, t+dt) for t in np.arange(tspan[0], tspan[1]+dt, dt)]
else:
    tspans = tspan

tspans = np.array(tspans)

if samples is not None:
    idx = np.random.choice(range(len(tspans)), samples, replace=False)
    tspans = tspans[idx]

# Convert to stereo coordinates
x, y = transform_coord(4326, proj, lon, lat)

# Subset data in space and time
time, x, y, obs = filter_domain(time, x, y, obs, radius=radius)

for q, tt in enumerate(tspans):

    time_, x_, y_, obs_ = filter_time(time, x, y, obs, tspan=tt)
    time_, x_, y_, obs_ = filter_invalid(time_, x_, y_, obs_)
    time_, x_, y_, obs_ = remove_nan(time_, x_, y_, obs_)

    if len(obs_) < 10:
        continue

    # Plot
    if 0:
        plt.scatter(x_, y_, c=obs_, s=1, rasterized=True,
                vmin=-np.nanstd(obs_), vmax=np.nanstd(obs_))
        plt.show()
        sys.exit()


    """ Calculate sample covariance. """

    print 'calculating pdist ...'
    t0 = tim()

    # Compute all distances between data points
    X = np.column_stack((x_, y_))
    dist = pdist(X, metric='euclidean')  # -> condensed dist_mat (vec)

    print 'time:', tim() - t0

    # Distance lags
    #lag = np.linspace(dmin, dmax, 100)
    lag = np.arange(dmin, dmax+dx, dx)

    # Half width of distance interval
    tol = dx/2.

    # Center
    obs_ -= np.nanmean(obs_)

    print 'calculating discov ...'
    t0 = tim()

    cov = distcov(obs_, obs_, dist, lag, tol)

    print 'time:', tim() - t0


    """ Save sample covariances. """

    if 1:
        path, ext = os.path.splitext(ifile)
        ofile = path + '.cov' + str(q)
        np.savetxt(ofile, np.column_stack((lag, cov)), fmt='%.6f')
        print 'file out ->', ofile

    """ Fit covariance model. """

    """
    # Remove NaNs
    ii, = np.where(~np.isnan(cov))
    cov = cov[ii]
    lag = lag[ii]

    c = cov             # dependent var
    r = lag             # independent var
    s = np.nanvar(obs)  # variance param [full data] (first guess)
    R = 1000.           # corr length param (first guess)

    rr = np.linspace(r.min(), r.max(), 100)

    # Fit models to data
    param, _ = curve_fit(gauss, r, c, p0=[s,R])
    g = gauss(rr, *param)
    print 'gauss', param

    param, _ = curve_fit(markov, r, c, p0=[s,R])
    m = markov(rr, *param)
    print 'markov', param

    param, _ = curve_fit(generic, r, c, p0=[s,R])
    v = generic(rr, *param)
    print 'generic', param
    """


    """ Plot results. """

    """
    plt.figure()
    plt.plot(r, c, 'o')
    plt.plot(rr, g, '-', linewidth=2.5, label='gauss')
    plt.plot(rr, m, '-', linewidth=2.5, label='markov')
    plt.plot(rr, v, '-', linewidth=2.5, label='generic')
    plt.title('Sample covariance and model fit, '+str(tt))
    plt.xlabel('Distance')
    plt.ylabel('Covariance')
    plt.legend()

    plt.figure()
    plt.plot(rr, m/s, '-', linewidth=2.5, label='corr func')
    plt.title('Correlation function')
    plt.xlabel('Distance')
    plt.ylabel('Correlation')
    plt.legend()

    plt.show()
    """

print 'done.'
