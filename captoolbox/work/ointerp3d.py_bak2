#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Spatial Optimal Interpolation using modeled covariance function.

Example:
     python ointerp.py ~/data/ers1/floating/filt_scat_det/joined_pts_ad.h5_ross -d 3 3 -n 100 -r 5 -e .1 -v t_year lon lat h_res None

"""

import os
import sys
import h5py
import pyproj
import argparse
import pandas as pd
import numpy as np
from datetime import datetime
from scipy.spatial import cKDTree
from numba import jit, int32, float64
from scipy.spatial.distance import cdist, pdist, squareform

import matplotlib.pyplot as plt

#-----------------------------------------------------------

""" Covariance models. """

# Modeled parameters
#s, R = [0.831433532, 1536.21967]  # gauss
s, R = [0.85417087, 655.42437697]  # markov  ##NOTE: Seems to be the best performing
#s, R = [0.720520448, 1084.11029]  # generic

# Half the time-range of horizontal layers
tau = 1.5/12.

def gauss(r, s, R):
    return s**2 * np.exp(-r**2/R**2)

def markov(r, s, R):
    return s**2 * (1 + r/R) * np.exp(-r/R)

def generic(r, s, R):
    return s**2 * (1 + (r/R) - 0.5 * (r/R)**2) * np.exp(-r/R)

def exp(t, tau):
    return np.exp(-t**2/tau**2)

def covxt(r, t, s, R, tau):
    """ C(r,t) = C(r) * C(t). """
    return markov(r, s, R) * exp(t, tau) 

#covmodel = markov
covmodel = covxt

#----------------------------------------------------------

def get_args():
    """ Get command-line arguments. """

    des = 'Optimal Interpolation of space-time data'
    parser = argparse.ArgumentParser(description=des)

    parser.add_argument(
            'ifile', metavar='ifile', type=str, nargs='+',
            help='name of i-file, numpy binary or ascii (for binary ".npy")')

    parser.add_argument(
            '-o', metavar='ofile', dest='ofile', type=str, nargs=1,
            help='name of o-file, numpy binary or ascii (for binary ".npy")',
            default=[None])

    parser.add_argument(
            '-i', metavar=('w','e','s','n'), dest='bbox', type=float, nargs=4,
            help=('bounding box for geograph. region (deg or m), optional'),
            default=[],)

    parser.add_argument(
            '-x', metavar=('x1', 'x2'), dest='xlim', type=float, nargs=2,
            help=('x/lon span to subest for covariance calc'),
            default=[None],)

    parser.add_argument(
            '-y', metavar=('y1', 'y2'), dest='ylim', type=float, nargs=2,
            help=('y/lat span to subest for covariance calc'),
            default=[None],)

    parser.add_argument(
            '-t', metavar=('t1', 't2'), dest='tlim', type=float, nargs=2,
            help=('time span to subest for covariance calc'),
            default=[None],)

    parser.add_argument(
            '-d', metavar=('dx','dy', 'dt'), dest='dxyt', type=float, nargs=3,
            help=('grid resolution in space and time (km km yr)'),
            default=[1, 1, 1],)

    parser.add_argument(
            '-n', metavar='nobs', dest='nobs', type=int, nargs=1,
            help=('number of obs. for each quadrant'),
            default=[100],)

    parser.add_argument(
            '-r', metavar='radius', dest='radius', type=float, nargs=1,
            help=('spatial search radius for each inversion cell (km)'),
            default=[1],)

    parser.add_argument(
            '-k', metavar='delta', dest='delta', type=float, nargs=1,
            help=('time range of data for each horizontal layer (yr)'),
            default=[1],)

    parser.add_argument(
            '-e', metavar='sigma', dest='sigma', type=float, nargs=1,
            help=('rms noise of obs. (m)'),
            default=[0],)

    parser.add_argument(
            '-v', metavar=('time', 'lon','lat', 'obs', 'err'), dest='vnames',
            type=str, nargs=5,
            help=('name of t/x/y/z/e variables in the HDF5. If err=None, skip'),
            default=[None], required=True)

    parser.add_argument(
            '-p', metavar=('epsg_num'), dest='proj', type=str, nargs=1,
            help=('EPSG proj number (AnIS=3031, GrIS=3413)'),
            default=['3031'],)

    return parser.parse_args()


def print_args(args):
    print 'Input arguments:'
    for arg in vars(args).iteritems():
        print arg


def mad_std(x, axis=None):
    """ Robust standard deviation (using MAD). """
    return 1.4826 * np.nanmedian(np.abs(x - np.nanmedian(x, axis)), axis)


def iterfilt(x, xmin, xmax, tol, alpha):
    """ Iterative outlier filter """
    
    # Set default value
    tau = 100.0
    
    # Remove data outside selected range
    x[x < xmin] = np.nan
    x[x > xmax] = np.nan
    
    # Initiate counter
    k = 0
    
    # Outlier rejection loop
    while tau > tol:
        
        # Compute initial rms
        rmse_b = mad_std(x)
        
        # Compute residuals
        dh_abs = np.abs(x - np.nanmedian(x))
        
        # Index of outliers
        io = dh_abs > alpha * rmse_b
        
        # Compute edited rms
        rmse_a = mad_std(x[~io])
        
        # Determine rms reduction
        tau = 100.0 * (rmse_b - rmse_a) / rmse_a
        
        # Remove data if true
        if tau > tol or k == 0:
            
            # Set outliers to NaN
            x[io] = np.nan
            
            # Update counter
            k += 1
    
    return x


def transform_coord(proj1, proj2, x, y):
    """
    Transform coordinates from proj1 to proj2 (EPSG num).

    Examples EPSG proj:
        Geodetic (lon/lat): 4326
        Stereo AnIS (x/y):  3031
        Stereo GrIS (x/y):  3413
    """
    # Set full EPSG projection strings
    proj1 = pyproj.Proj("+init=EPSG:"+str(proj1))
    proj2 = pyproj.Proj("+init=EPSG:"+str(proj2))
    # Convert coordinates
    return pyproj.transform(proj1, proj2, x, y)


def get_grid(xmin, xmax, ymin, ymax, dx, dy):
    """ Generate a regular grid. """

    # Setup grid dimensions
    Nx = int((np.abs(xmax - xmin)) / dx) + 1
    Ny = int((np.abs(ymax - ymin)) / dy) + 1

    # Initiate lat/lon vectors for grid
    x = np.linspace(xmin, xmax, Nx)
    y = np.linspace(ymin, ymax, Ny)

    # Construct output grid-coordinates
    return np.meshgrid(x, y)


def time_grid(tmin, tmax, dt):
    Nt = int((np.abs(tmax - tmin)) / dt) + 1
    return np.linspace(tmin, tmax, Nt)


def get_segments(time, tmax=1):
    """
    Partition time array into segments with breaks > tmax.

    Returns an array w/unique identifiers for each segment.
    """
    n = 0
    trk = np.zeros(time.shape)
    for k in xrange(1, len(time)):
        if np.abs(time[k]-time[k-1]) > tmax:
            n += 1
        trk[k] = n
    return trk


""" Compiled functions. """

@jit(nopython=True)
def add_offdiag_err(A, B, C, err):
    """
    Add correlated (off-diagonal) errors to C.

    If i,j belong to the same track (aij == bij)
    and they are not in the diagonal (i != j), then:
        cij += sigma
    """
    M, N = A.shape
    for i in range(M):
        for j in range(N):
            aij = A[i,j]
            bij = B[i,j]
            if i != j and aij == bij:
                C[i,j] += err


@jit(nopython=True)
def space_dist_grid_data(x0, y0, x, y):
    return np.sqrt((x-x0) * (x-x0) + (y-y0) * (y-y0))


def time_dist_grid_data(t0, tc):
    """ Compute time distance between prediction pt and obs. """
    return np.abs(tc - t0)


def space_dist_data_data(x, y):
    """ Compute spatial distances between obs. """
    X = np.column_stack((x, y))
    return cdist(X, X, "euclidean")


def time_dist_data_data(t):
    """ Compute time distances between obs. """
    X = t[:,np.newaxis]
    return np.abs(cdist(X, X, "euclidean"))

#-------------

""" Helper functions. """

def subset_data(t, x, y, z,
        tlim=(1995.25, 1995.5), xlim=(-1, 1), ylim=(-1, 1)):
    """ Subset data domain (add NaNs). """
    tt = (t >= tlim[0]) & (t <= tlim[1])
    xx = (x >= xlim[0]) & (x <= xlim[1])
    yy = (y >= ylim[0]) & (y <= ylim[1])
    ii, = np.where(tt & xx & yy)
    return t[ii], x[ii], y[ii], z[ii]


def remove_invalid(t, x, y, z):
    """ Remove NaNs and Zeros. """
    #ii, = np.where((z != 0) & ~np.isnan(z))
    ii, = np.where(~np.isnan(z))
    return t[ii], x[ii], y[ii], z[ii]


def mad_std(x, axis=None):
    """ Robust standard deviation (using MAD). """
    return 1.4826 * np.nanmedian(np.abs(x - np.nanmedian(x, axis)), axis)


def iterfilt(x, xmin, xmax, tol, alpha):
    """ Iterative outlier filter """
    
    # Set default value
    tau = 100.0
    
    # Remove data outside selected range
    x[x < xmin] = np.nan
    x[x > xmax] = np.nan
    
    # Initiate counter
    k = 0
    
    # Outlier rejection loop
    while tau > tol:
        
        # Compute initial rms
        rmse_b = mad_std(x)
        
        # Compute residuals
        dh_abs = np.abs(x - np.nanmedian(x))
        
        # Index of outliers
        io = dh_abs > alpha * rmse_b
        
        # Compute edited rms
        rmse_a = mad_std(x[~io])
        
        # Determine rms reduction
        tau = 100.0 * (rmse_b - rmse_a) / rmse_a
        
        # Remove data if true
        if tau > tol or k == 0:
            
            # Set outliers to NaN
            x[io] = np.nan
            
            # Update counter
            k += 1
    
    return x


def center_tracks(z, trk, median=False):
    """ Center each individual track. """
    if median:
        mean = np.nanmedian(z)
    else:
        mean = np.nanmean(z)
    for k in np.unique(trk): 
        i_trk, = np.where(trk == k)
        if median:
            z[i_trk] -= np.nanmedian(z[i_trk])
        else:
            z[i_trk] -= np.nanmean(z[i_trk])
    return z, mean


##TODO: Implement the 'percent'
def random(x, n, percent=False):
    """ Draw random indices from array. """
    N = len(x)
    if N > n:
        i_rand = np.random.choice(np.arange(N), n, replace=False)
    else:
        i_rand = range(N)
    return i_rand


#---------------------------------------------------

# Parser argument to variable
args = get_args() 

# Read input from terminal
ifile = args.ifile[0]
ofile = args.ofile[0]
bbox = args.bbox
tvar = args.vnames[0]
xvar = args.vnames[1]
yvar = args.vnames[2]
zvar = args.vnames[3]
evar = args.vnames[4]
tlim = args.tlim[:]
xlim = args.xlim[:]
ylim = args.ylim[:]
dx = args.dxyt[0] * 1e3
dy = args.dxyt[1] * 1e3
dt = args.dxyt[2]
radius = args.radius[0] * 1e3
delta = args.delta[0]
sigma = args.sigma[0]
proj = args.proj[0]

# Print parameters to screen
print_args(args)

# Start timing of script
startTime = datetime.now()

print "reading input file ..."

with h5py.File(ifile, 'r') as f:

    step = 1                    ##FIXME: Set this for final version
    time = f[tvar][::step]
    lon = f[xvar][::step]
    lat = f[yvar][::step]
    obs = f[zvar][::step]
    err = f[evar][::step] if evar != 'None' else np.full_like(obs, None)

    if 1:
        # Remove uncorrected data (this should be done before applying this code?)
        b = f['h_bs'][::step] 
        obs[b==0] = np.nan
        obs[np.isnan(b)] = np.nan


if None in tlim:
    tlim = [np.nanmin(time), np.nanmax(time)]

if None in xlim:
    xlim = [np.nanmin(lon), np.nanmax(lon)]

if None in ylim:
    ylim = [np.nanmin(lat), np.nanmax(lat)]

if ofile is None:
    ofile = ifile + '_interp'

# Include the 'delta time' and 'search radius' for subsetting the data 
tlim_ = tlim[:]
xlim_ = xlim[:]
ylim_ = ylim[:]
tlim_[0] -= delta/2.
tlim_[1] += delta/2.
xlim_[0] -= radius/100000.
xlim_[1] += radius/100000.
ylim_[0] -= radius/100000.
ylim_[1] += radius/100000.

# Subset data in space and time
time, lon, lat, obs = subset_data(time, lon, lat, obs, tlim=tlim_, xlim=xlim_, ylim=ylim_)
time, lon, lat, obs = remove_invalid(time, lon, lat, obs)

if len(obs) < 100:
    print 'not sufficient data points!'
    sys.exit()

# Convert to stereo coordinates
x, yp_ = transform_coord(4326, proj, lon, lat)

# Extract observations
obs = obs
time = time

# Assign a track ID to each data point
print 'separating tracks ...'
tsec = time * 3.154e7  # year -> sec
kp = get_segments(tsec, tmax=100)

# Plot
if 0:
    plt.figure()
    plt.scatter(xp, yp, c=obs, s=1, rasterized=True,
            vmin=-np.nanstd(obs)/2., vmax=np.nanstd(obs)/2.,
            cmap=plt.cm.RdBu)
    plt.colorbar()

    plt.figure()
    trk_unique = np.unique(kp)
    for k in trk_unique:
        ii, = np.where(k == kp)
        x = xp[ii]
        yp_ = yp[ii]
        obs = zp[ii]

        xsub_ = xp[ii[::3]]
        yp_sub_ = yp[ii[::3]]
        obssub_ = zp[ii[::3]]

        # Plot all tracks
        plt.plot(x, yp_, '.', rasterized=True)
        '''
        # Plot individual track profiles
        plt.plot(np.hypot(x, yp_), obs, '.')
        plt.plot(np.hypot(xsub_, yp_sub_), obssub_, '.')
        plt.show()
        '''
    plt.show()
    sys.exit()


# Spatial limits of the prediction grid 
if len(bbox) == 6:
    # Extract bounding box elements
    xmin, xmax, ymin, ymax = bbox
else:
    # Create bounding box limits
    xmin, xmax, ymin, ymax = (x.min() - 10.*dx), (x.max() + 10.*dx), \
                             (yp_.min() - 10.*dy), (yp_.max() + 10.*dy)

# Time limits of the prediction grid
tmin, tmax = tlim

#----- Prediction grid
##TODO: Here pass the prediction grid

# Generate 2D prediction grid     
print 'generating grid ...'
Xi, Yi = get_grid(xmin, xmax, ymin, ymax, dx, dy)

# Generate time steps for each horizontal slice 
ti = time_grid(tmin, tmax, dt)

# Flatten prediction grid
xi, yi = Xi.ravel(), Yi.ravel()

#----- Prediction grid

# Output vectors
zi = np.full(xi.shape[0], np.nan)
ei = np.full(xi.shape[0], np.nan)
ni = np.full(xi.shape[0], np.nan)

# Output 3D arrays
zzz = np.full((Xi.shape[0],Xi.shape[1],ti.shape[0]), np.nan)
eee = np.full((Xi.shape[0],Xi.shape[1],ti.shape[0]), np.nan)
nnn = np.full((Xi.shape[0],Xi.shape[1],ti.shape[0]), np.nan)

print 'transforming coords ...'
# Geographical projection
if np.abs(ymax) < 100:
    
    # Convert to stereographic coord.
    (xi, yi) = pyproj.transform(projGeo, projGrd, xi, yi)

# Compute noise variance
sigma2 = sigma * sigma

# Compute Long-wavelength (along-track) error
sigma2_L = sigma2 * 0.5  ##FIXME: What's a good error?!!!

# Half width of horizontal layer => plus-minus delta_t
h = delta/2.

##TODO: Parallelize here!!!

# Loop though each time step
for i_step, t_step in enumerate(ti):


    print 'time step/limits:', t_step, t_step-h, t_step+h

    i_slice, = np.where( (time >= t_step-h) & (time <= t_step+h) )
    tp = time[i_slice]
    xp = x[i_slice]
    yp = yp_[i_slice]
    zp = obs[i_slice]

    if len(i_slice) < 10:
        continue

    print "setting up kd-tree ..."

    # Construct cKDTree (one per vertical layer)
    TreeP = cKDTree(np.column_stack((xp, yp)))

    print "looping grid nodes ..."

    # Remove mean bacground field (and restore it later)
    zmean = np.nanmean(zp)
    #zp -= zmean


def interp2d(data, grid, tstep=None):

    tp, xp, yp, zp, kp = data
    xi, yi = grid
    ti = tstep

    if ti is None:
        # Center the time reference for 2d field
        ti = tp.min() + (tp.max()-tp.min()/2.

    # Enter prediction loop
    for i_node in xrange(xi.shape[0]):

        # Get prediction pt (grid node)
        t0 = ti[i_step]
        x0 = xi[i_node]
        y0 = yi[i_node]

        # Get data within search radius 
        i_cell = TreeP.query_ball_point((x0, y0), radius)
        tc = tp[i_cell]
        xc = xp[i_cell]
        yc = yp[i_cell]
        zc = zp[i_cell]
        kc = kp[i_cell]

        # Quick outlier editing
        zmin, zmax, tol, thres = [-5, 5, 5, 3]
        i_filt = ~np.isnan(iterfilt(zc.copy(), zmin, zmax, tol, thres))
        tc = tc[i_filt]
        xc = xc[i_filt]
        yc = yc[i_filt]
        zc = zc[i_filt]
        kc = kc[i_filt]

        # Thin the data (randomly)
        if 0:
            i_rand = random(zc, 25)
            tc = tc[i_rand]
            xc = xc[i_rand]
            yc = yc[i_rand]
            zc = zc[i_rand]
            kc = kc[i_rand]

        # Test for empty cell
        if len(zc) < 50:
            continue

        # Remove offset from endividual tracks
        #zc, cell_mean = center_tracks(zc, kc, median=False)  ##FIXME: To do or not to do?

        # Plot data within search radius
        if 0:

            plt.figure()
            plt.scatter(xc, yc, c=zc, s=50, cmap=plt.cm.RdBu)

            plt.figure()
            k_unique = np.unique(kc)

            for ki in k_unique:

                i_trk, = np.where(ki == kc)

                plt.plot(np.hypot(xc[i_trk], yc[i_trk]), zc[i_trk], 'o')
                plt.xlabel('Distance along track (m)')

                zc[i_trk] -= np.nanmean(zc[i_trk])

                plt.plot(np.hypot(xc[i_trk], yc[i_trk]), zc[i_trk], 'x')
                plt.xlabel('Distance along track (m)')

            plt.figure()
            plt.scatter(xc, yc, c=zc, s=50, cmap=plt.cm.RdBu)

            plt.show()
            continue

        # Compute spatial distance between prediction pt and obs in search radius
        Dxj = space_dist_grid_data(x0, y0, xc, yc)  # -> vec

        # Compute time distance between prediction pt and obs in search radius
        Dxk = time_dist_grid_data(t0, tc)  # -> vec

        # Compute spatial distances between obs in search radius 
        Dij = space_dist_data_data(xc, yc)  # -> mat

        # Compute time distances between obs in search radius 
        Dik = time_dist_data_data(tc)  # -> mat
        
        """ Build Error matrix. """

        # Build error matrix 
        if np.isnan(err).all():
            # Provide all obs. the same sigma
            ec = np.full(xc.shape[0], sigma2)

        else:
            # Set all obs. errors < sigma2 to sigma2
            ec = err[Io]**2
            ec[ec<sigma2] = sigma2

        # Estimate local median (robust) and local variance of data
        m0 = np.nanmedian(zc)
        #m0 = np.nanmean(zc)
        c0 = np.nanvar(zc) 

        # Scaling factor to convert: global cov -> local cov
        scale = c0/covmodel(0, 0, s, R, tau)
        
        # Covariance vector: model-data 
        Cxj = covmodel(Dxj, Dxk, s, R, tau) * scale  ##FIXME
        #Cxj = covmodel(Dxj, 0, s, R, tau) * scale
        
        # Covariance matrix: data-data 
        #Cij = covmodel(Dij, Dik, s, R, tau) * scale  ##FIXME
        Cij = covmodel(Dij, 0., s, R, tau) * scale

        ######

        if 0:
            # Plot cov values vs space-time distance
            plt.figure()
            plt.scatter(Dxj, Cxj, c=Dxk, s=20, cmap=plt.cm.hot)
            plt.colorbar()
            plt.title('Cov(grid,data) vs space-time dist')
            plt.xlabel('Spatial distance')
            plt.ylabel('Covariance')
            plt.figure()
            plt.scatter(Dij, Cij, c=Dik, s=20, cmap=plt.cm.hot)
            plt.colorbar()
            plt.title('Cov(data,data) vs space-time dist')
            plt.xlabel('Spatial distance')
            plt.ylabel('Covariance')
            plt.show()
            continue

        n_obs = zc.shape[0]

        # Uncorrelated errors
        # (diagonal => variance of uncorrelated white noise)
        Nij = np.diag(ec)  

        # Matrices with track id for each data point
        Kx, Ky = np.meshgrid(kc, kc)

        # Plot error matrix w/diagonal only
        if 0:
            plt.figure()
            plt.imshow(Nij)

        # Correlated errors
        # (off-diagonal => variance of along-track long-wavelength error)
        if 1:
            add_offdiag_err(Kx, Ky, Nij, sigma2_L)

        # Plot error matrix w/off-diagonal entries
        if 0:
            plt.figure()
            plt.imshow(Nij)
            plt.show()
            continue

        """ Solve the Least-Squares system for the inversion cell. """

        # Augmented data-cov matrix w/errors
        Aij = Cij + Nij

        # Matrix inversion of: Cxj * Aij^(-1)
        CxjAiji = np.linalg.solve(Aij.T, Cxj.T)

        # Predicted value
        zi[i_node] = np.dot(CxjAiji, zc) + (1 - np.sum(CxjAiji)) * m0
        
        # Predicted error
        ei[i_node] = np.sqrt(np.abs(c0 - np.dot(CxjAiji, Cxj.T)))
        
        # Number of data used for prediction    
        ni[i_node] = len(zc)

        # Print progress to terminal
        if (i_node % 500) == 0:
            
            # N-predicted values
            print str(i_node) + '/' + str(len(xi)) \
                    + ' Pred: ' + str(np.around(zi[i_node],2)) \
                    + '  Nsol: '+ str(ni[i_node]) \
                    + '  Dmax: ' + str(np.around(1e-3 * Dxj.max(),2))

            print 'Inversion Cell Times:', tc.min(), tc.max()


        # Restore the cell mean value to prediction
        #zi[i_node] += cell_mean
        #zi[i_node] = cell_mean


    # Convert back to arrays
    zz = np.flipud(zi.reshape(Xi.shape))
    ee = np.flipud(ei.reshape(Xi.shape))
    nn = np.flipud(ni.reshape(Xi.shape))

    # Store horizontal layer
    zzz[:,:,i_step] = zz
    eee[:,:,i_step] = ee
    nnn[:,:,i_step] = nn

""" Save interpolated fields. """

# Save prediction
if 1:
    with h5py.File(ofile, 'w') as f:
        f['t'] = ti
        f['x'] = xx
        f['y'] = yy
        f['z'] = zzz
        f['e'] = eee
        f['n'] = nnn

    print 'output ->', ofile

""" Plot interpolated fields. """

if 1:

    from scipy import ndimage as ndi

    xx = np.flipud(Xi)
    yy = np.flipud(Yi)

    for k in range(zzz.shape[2]):

        t_k = ti[k]
        i_slice, = np.where( (time >= t_k-h) & (time <= t_k+h) )
        tp = time[i_slice]
        xp = x[i_slice]
        yp = yp_[i_slice]
        zp = obs[i_slice]
        zp -= np.nanmean(zp)

        zz = zzz[:,:,k]
        zz -= np.nanmean(zz)

        # Filter spatial field
        if 1:
            zz = ndi.median_filter(zz, 3)

        vmin = -.4
        vmax = .4

        plt.figure()
        plt.pcolormesh(xx, yy, zz, vmin=vmin, vmax=vmax, cmap=plt.cm.RdBu)
        plt.colorbar()

        plt.figure()
        plt.scatter(xp, yp, c=zp, s=1, vmin=vmin, vmax=vmax, cmap=plt.cm.RdBu)
        plt.colorbar()

    plt.show()


# Print execution time of script
print 'Execution time: '+ str(datetime.now()-startTime)
